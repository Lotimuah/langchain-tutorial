{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic chain\n",
    "\n",
    "1. Prompt\n",
    "   - `from_template`\n",
    "2. Model\n",
    "   - `ChatOpenAI`\n",
    "3. Parser\n",
    "   - `StrOutputParser`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the bear dissolve its business?  \\nBecause it was in the red!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function calling\n",
    "\n",
    "1. Prompt\n",
    "   - `from_messages`\n",
    "2. Model\n",
    "   - `ChatOpenAI`\n",
    "   - `functions`\n",
    "3. Parser\n",
    "   - `OpenAIFunctionsAgentOutputParser`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define input class(`pydantic`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class WikipediaInput(BaseModel):\n",
    "    query: str = Field(description=\"Query to search\")\n",
    "    n_contents: int = Field(description=\"Number of contents to return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define python function(`tool`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from langchain.agents import tool\n",
    "from wikipedia.exceptions import PageError, DisambiguationError\n",
    "\n",
    "\n",
    "@tool(args_schema=WikipediaInput)\n",
    "def search_wikipedia(query: str, n_contents: int) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "\n",
    "    for title in titles[:n_contents]:\n",
    "        try:\n",
    "            page = wikipedia.page(title=title, auto_suggest=False)\n",
    "            summary = page.summary\n",
    "            summaries.append(f\"[{title}]\\n\" f\"{summary}\")\n",
    "        except (PageError, DisambiguationError):\n",
    "            pass\n",
    "\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "\n",
    "    return (50 * \"=\").join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24865/1765610969.py:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  search_wikipedia({'query': \"langchain\", 'n_contents': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[LangChain]\\nLangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia({\"query\": \"langchain\", \"n_contents\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Convert to OpenAI function(schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'search_wikipedia',\n",
       "  'description': 'Run Wikipedia search and get page summaries.',\n",
       "  'parameters': {'properties': {'query': {'description': 'Query to search',\n",
       "     'type': 'string'},\n",
       "    'n_contents': {'description': 'Number of contents to return',\n",
       "     'type': 'integer'}},\n",
       "   'required': ['query', 'n_contents'],\n",
       "   'type': 'object'}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "tools = [search_wikipedia]\n",
    "# tools = [WikipediaInput]  # also possible\n",
    "functions = [convert_to_openai_function(tool) for tool in tools]\n",
    "functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Build model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Bind function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "model = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Bind function + Forcing not to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "model = model.bind(functions=functions, function_call=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Bind function + Forcing\n",
    "\n",
    "- 다른 방법들에 비해 `prompt_tokens` 개수가 가장 많으니 주의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "model = model.bind(functions=functions, function_call={\"name\": \"search_wikipedia\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Build chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool='search_wikipedia' tool_input={'query': 'Hello', 'n_contents': 1} log=\"\\nInvoking: `search_wikipedia` with `{'query': 'Hello', 'n_contents': 1}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Hello\",\"n_contents\":1}', 'name': 'search_wikipedia'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 85, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BFu0i6IlbYXNABHEAT10yKdzNGmBN', 'finish_reason': 'stop', 'logprobs': None}, id='run-0eed2cf8-5e74-49e4-9420-bf445aad2e45-0', usage_metadata={'input_tokens': 85, 'output_tokens': 11, 'total_tokens': 96, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "tool='search_wikipedia' tool_input={'query': 'Langchain', 'n_contents': 1} log=\"\\nInvoking: `search_wikipedia` with `{'query': 'Langchain', 'n_contents': 1}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Langchain\",\"n_contents\":1}', 'name': 'search_wikipedia'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 88, 'total_tokens': 100, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BFu0jmc47eV6rN01hZQYPlp9M8Q36', 'finish_reason': 'stop', 'logprobs': None}, id='run-888bea6b-3817-4f6f-9cd4-7483ef042cbb-0', usage_metadata={'input_tokens': 88, 'output_tokens': 12, 'total_tokens': 100, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents.output_parsers.openai_functions import (\n",
    "    OpenAIFunctionsAgentOutputParser,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are helpful but sassy assistant\"), (\"user\", \"{input}\")]\n",
    ")\n",
    "parser = OpenAIFunctionsAgentOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "print(chain.invoke({\"input\": \"Hi!\"}))\n",
    "print(chain.invoke({\"input\": \"what is langchain?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example) Tagging & Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Tagging\n",
    "\n",
    "텍스트로부터 구조화된 데이터를 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentiment': 'pos', 'language': 'en'},\n",
       " {'sentiment': 'neg', 'language': 'it'},\n",
       " {'sentiment': 'neutral', 'language': 'ko'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "\n",
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of the text with particular info.\"\"\"\n",
    "\n",
    "    sentiment: str = Field(\n",
    "        description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\"\n",
    "    )\n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI().bind(\n",
    "    functions=[convert_to_openai_function(Tagging)], function_call={\"name\": \"Tagging\"}\n",
    ")\n",
    "parser = JsonOutputFunctionsParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.batch(\n",
    "    (\n",
    "        {\"input\": \"I love langchain\"},\n",
    "        {\"input\": \"non mi piace questo cibo\"},\n",
    "        {\"input\": \"이 식당은 밥을 적정량 준다\"},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Extraction\n",
    "\n",
    "Tagging과 비슷하지만 **다수**의 정보를 추출한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'people': [{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': None}]},\n",
       " {'people': [{'name': '민철', 'age': 30}, {'name': '희진', 'age': 50}]}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(description=\"person's name\")\n",
    "    age: Optional[int] = Field(description=\"person's age\")\n",
    "\n",
    "\n",
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "\n",
    "    people: List[Person] = Field(description=\"List of info about people\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Extract the relevant information, if not explicitly provided do not guess and stay empty. Extract partial info\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI().bind(\n",
    "    functions=[convert_to_openai_function(Information)],\n",
    "    function_call={\"name\": \"Information\"},\n",
    ")\n",
    "parser = JsonOutputFunctionsParser(key_name=\"people\")\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.batch(\n",
    "    (\n",
    "        {\"input\": \"Joe is 30, his mom is Martha who is 20 years older than Joe\"},\n",
    "        {\"input\": \"민철이는 30살이고, 그보다 20살 많은 어머니의 이름은 희진입니다\"},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Document extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.web_base import WebBaseLoader\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "documents = loader.load()\n",
    "doc = documents[0]\n",
    "doc.page_content[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'This document discusses building agents with LLM as the core controller in autonomous agent systems. It covers components like planning, memory, and tool use, along with subtopics such as task decomposition, self-reflection, and challenges faced in LLM-powered autonomous agents. Various methodologies and frameworks like Chain of Hindsight and Algorithm Distillation are explored to enhance agent performance.',\n",
       " 'language': 'English',\n",
       " 'keywords': 'LLM, autonomous agent system, planning, memory, tool use, task decomposition, self-reflection, Chain of Hindsight, Algorithm Distillation'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "\n",
    "class Overview(BaseModel):\n",
    "    \"\"\"Overview of a section of text.\"\"\"\n",
    "\n",
    "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
    "    language: str = Field(\n",
    "        description=\"Provide the language that the content is written.\"\n",
    "    )\n",
    "    keywords: str = Field(description=\"Provide keywords.\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are helpful summary expert for document. Extract the information from the given text.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI().bind(\n",
    "    functions=[convert_to_openai_function(Overview)], function_call={\"name\": \"Overview\"}\n",
    ")\n",
    "chain = prompt | model | JsonOutputFunctionsParser()\n",
    "\n",
    "chain.invoke({\"input\": doc.page_content[:10000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chain of thought (CoT; Wei et al. 2022)',\n",
       "  'author': 'Wei et al. 2022'},\n",
       " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'LLM+P (Liu et al. 2023)', 'author': 'Liu et al. 2023'},\n",
       " {'title': 'ReAct (Yao et al. 2023)', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': 'Shinn & Labash 2023'},\n",
       " {'title': 'Chain of Hindsight (CoH; Liu et al. 2023)',\n",
       "  'author': 'Liu et al. 2023'},\n",
       " {'title': 'Algorithm Distillation (AD; Laskin et al. 2023)',\n",
       "  'author': 'Laskin et al. 2023'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "\n",
    "class Paper(BaseModel):\n",
    "    \"\"\"Information about papers mentioned.\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"Title of the paper without authors\")\n",
    "    author: Optional[str] = Field(description=\"Author of the paper\")\n",
    "\n",
    "\n",
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "\n",
    "    papers: List[Paper] = Field(description=\"List of papers to extract from the text\")\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "A article will be passed to you. Extract from it all papers that are mentioned by this article.\n",
    "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
    "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
    "model = ChatOpenAI(temperature=0).bind(\n",
    "    functions=[convert_to_openai_function(Information)],\n",
    "    function_call={\"name\": \"Information\"},\n",
    ")\n",
    "chain = prompt | model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\n",
    "\n",
    "chain.invoke({\"input\": doc.page_content[:10000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Information',\n",
       " 'description': 'Information to extract.',\n",
       " 'parameters': {'properties': {'papers': {'description': 'List of papers to extract from the text',\n",
       "    'items': {'description': 'Information about papers mentioned.',\n",
       "     'properties': {'title': {'description': 'Title of the paper without authors',\n",
       "       'type': 'string'},\n",
       "      'author': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'description': 'Author of the paper'}},\n",
       "     'required': ['title', 'author'],\n",
       "     'type': 'object'},\n",
       "    'type': 'array'}},\n",
       "  'required': ['papers'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_openai_function(Information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 긴 문서는 잘라서 넣고 나중에 합친다.\n",
    "  - `langchain.text_splitter.RecursiveCharacterTextSplitter`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'AutoGPT', 'author': None},\n",
       " {'title': 'GPT-Engineer', 'author': None},\n",
       " {'title': 'BabyAGI', 'author': None},\n",
       " {'title': 'Chain of thought', 'author': 'Wei et al. 2022'},\n",
       " {'title': 'Tree of Thoughts', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'LLM+P', 'author': 'Liu et al. 2023'},\n",
       " {'title': 'ReAct', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'Reflexion', 'author': 'Shinn & Labash 2023'},\n",
       " {'title': 'Chain of Hindsight (CoH)', 'author': 'Liu et al. 2023'},\n",
       " {'title': 'Algorithm Distillation (AD)', 'author': 'Laskin et al. 2023'},\n",
       " {'title': 'Laskin et al. 2023', 'author': None},\n",
       " {'title': 'Miller 1956', 'author': None},\n",
       " {'title': 'Duan et al. 2017', 'author': None},\n",
       " {'title': 'LSH (Locality-Sensitive Hashing)', 'author': None},\n",
       " {'title': 'ANNOY (Approximate Nearest Neighbors Oh Yeah)', 'author': None},\n",
       " {'title': 'HNSW (Hierarchical Navigable Small World)', 'author': None},\n",
       " {'title': 'FAISS (Facebook AI Similarity Search)', 'author': None},\n",
       " {'title': 'ScaNN (Scalable Nearest Neighbors)', 'author': None},\n",
       " {'title': 'MRKL (Karpas et al. 2022)', 'author': 'Karpas et al.'},\n",
       " {'title': 'TALM (Tool Augmented Language Models; Parisi et al. 2022)',\n",
       "  'author': 'Parisi et al.'},\n",
       " {'title': 'Toolformer (Schick et al. 2023)', 'author': 'Schick et al.'},\n",
       " {'title': 'HuggingGPT (Shen et al. 2023)', 'author': 'Shen et al.'},\n",
       " {'title': 'API-Bank', 'author': 'Li et al. 2023'},\n",
       " {'title': 'ChemCrow', 'author': 'Bran et al. 2023'},\n",
       " {'title': 'Boiko et al. (2023)', 'author': 'Boiko et al.'},\n",
       " {'title': 'Generative Agents Simulation', 'author': 'Park, et al. 2023'},\n",
       " {'title': 'Chain of thought prompting elicits reasoning in large language models',\n",
       "  'author': 'Wei et al.'},\n",
       " {'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Chain of Hindsight Aligns Language Models with Feedback',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'ReAct: Synergizing reasoning and acting in language models',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Reflexion: an autonomous agent with dynamic memory and self-reflection',\n",
       "  'author': 'Shinn & Labash'},\n",
       " {'title': 'In-context Reinforcement Learning with Algorithm Distillation',\n",
       "  'author': 'Laskin et al.'},\n",
       " {'title': 'MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning',\n",
       "  'author': 'Karpas et al.'},\n",
       " {'title': 'Webgpt: Browser-assisted question-answering with human feedback',\n",
       "  'author': 'Nakano et al.'},\n",
       " {'title': 'TALM: Tool Augmented Language Models', 'author': 'Parisi et al.'},\n",
       " {'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools',\n",
       "  'author': 'Schick et al.'},\n",
       " {'title': 'API-Bank: A Benchmark for Tool-Augmented LLMs',\n",
       "  'author': 'Li et al.'},\n",
       " {'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace',\n",
       "  'author': 'Shen et al.'},\n",
       " {'title': 'ChemCrow: Augmenting large-language models with chemistry tools',\n",
       "  'author': 'Bran et al.'},\n",
       " {'title': 'Emergent autonomous scientific research capabilities of large language models',\n",
       "  'author': 'Boiko et al.'},\n",
       " {'title': 'Generative Agents: Interactive Simulacra of Human Behavior',\n",
       "  'author': 'Joon Sung Park, et al.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "def flatten(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list += row\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)\n",
    "splits = text_splitter.split_text(doc.page_content)\n",
    "prep = RunnableLambda(lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)])\n",
    "split_chain = prep | chain.map() | flatten\n",
    "\n",
    "split_chain.invoke(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Batch processing\n",
    "\n",
    "가능한 batch 처리되는 프로세스들은 병렬적으로 실행된다. (default `n_procs=5`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why did the bear break up with his girlfriend?\\nBecause she couldn't bear his terrible puns!\",\n",
       " 'Why was the frog so happy?\\nBecause he ate a fly and finally got to put something in his \"toad\"stool!']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Streaming processing\n",
    "\n",
    "적은 지연시간은 더 나은 사용자 경험을 만들어낼 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why\n",
      " did\n",
      " the\n",
      " bear\n",
      " bring\n",
      " a\n",
      " flashlight\n",
      " to\n",
      " the\n",
      " party\n",
      "?\n",
      " \n",
      "\n",
      "\n",
      "Because\n",
      " he\n",
      " wanted\n",
      " to\n",
      " \"\n",
      "light\n",
      " up\n",
      "\"\n",
      " the\n",
      " dance\n",
      " floor\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "for t in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Asynchronous methods\n",
    "\n",
    "용도에 따라 비동기적 처리도 가능 (`ainvoke`, `abatch`, `astream`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the bear break up with his girlfriend? \\n\\nBecause she was too grizzly for him!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Routing\n",
    "\n",
    "사용되는 tool에 따라 다른 방식으로 처리할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "\n",
    "class PositiveInput(BaseModel):\n",
    "    persons: List[str] = Field(\n",
    "        description=\"Name list of persons having positive action\"\n",
    "    )\n",
    "\n",
    "\n",
    "@tool(args_schema=PositiveInput)\n",
    "def positive(persons: List[str]) -> str:\n",
    "    \"\"\"Handle persons having only positive action.\"\"\"\n",
    "    return f\"[Positive] {persons}\"\n",
    "\n",
    "\n",
    "class NegativeInput(BaseModel):\n",
    "    persons: List[str] = Field(\n",
    "        description=\"Name list of persons having negative action\"\n",
    "    )\n",
    "\n",
    "\n",
    "@tool(args_schema=NegativeInput)\n",
    "def negative(persons: List[str]) -> str:\n",
    "    \"\"\"Handle persons having only negative action.\"\"\"\n",
    "    return f\"[Negative] {persons}\"\n",
    "\n",
    "\n",
    "class NeutralInput(BaseModel):\n",
    "    persons: List[str] = Field(description=\"Name list of persons having neutral action\")\n",
    "\n",
    "\n",
    "@tool(args_schema=NeutralInput)\n",
    "def neutral(persons: List[str]) -> str:\n",
    "    \"\"\"Handle persons having only neutral action.\"\"\"\n",
    "    return f\"[Neutral] {persons}\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are expert of discrimating persons with emotional action. Positive or negative or neutral.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(temperature=0).bind(\n",
    "    functions=[convert_to_openai_function(fn) for fn in [positive, negative, neutral]]\n",
    ")\n",
    "parser = OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive {'persons': ['Jimmy']}\n",
      "negative {'persons': ['Sansu']}\n"
     ]
    }
   ],
   "source": [
    "result = chain.batch(\n",
    "    [\n",
    "        {\"input\": \"Jimmy helps the disabled\"},\n",
    "        {\"input\": \"Sansu stole the money from the bank\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result[0].tool, result[0].tool_input)\n",
    "print(result[1].tool, result[1].tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "\n",
    "def route(result):\n",
    "    if isinstance(result, AgentFinish):\n",
    "        return result.return_values[\"output\"]\n",
    "    else:\n",
    "        return eval(result.tool).run(result.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser | route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[Positive] ['Jimmy', 'Sansu']\",\n",
       " \"I'm just a computer program, so I don't have feelings, but I'm here to help you. How can I assist you today?\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch(\n",
    "    [\n",
    "        {\"input\": \"Jimmy like to eat vegatables and Sansu wants to listen piano.\"},\n",
    "        {\"input\": \"Hi, how are you?\"},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Positive] ['Hammy', 'Minsu']\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"input\": \"\"\"\n",
    "              Jimmy does not eat vegatables but Hammy likes to eat vegatables.\n",
    "              Minsu likes to play piano but Sansu hates to listen it.\n",
    "              \"\"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한 문장 안에서 여러 개의 tool이 사용되는 경우는 처리하지 못하는 모양\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. ChatBot\n",
    "\n",
    "Multi-turn 대화를 위해 `placeholder`, `memory`를 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from wikipedia.exceptions import PageError, DisambiguationError\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "\n",
    "class WikipediaInput(BaseModel):\n",
    "    query: str = Field(description=\"Query to search\")\n",
    "\n",
    "\n",
    "@tool(args_schema=WikipediaInput)\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "\n",
    "    for title in titles:\n",
    "        try:\n",
    "            page = wikipedia.page(title=title, auto_suggest=False)\n",
    "            summary = page.summary\n",
    "            summaries.append(f\"[{title}]\\n\" f\"{summary}\")\n",
    "            break\n",
    "        except (PageError, DisambiguationError):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return (50 * \"=\").join(summaries)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are helpful but sassy assistant\"), (\"user\", \"{input}\")]\n",
    ")\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\").bind(\n",
    "    functions=[convert_to_openai_function(search_wikipedia)]\n",
    ")\n",
    "parser = OpenAIFunctionsAgentOutputParser()\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'Well, hello there! What can I do for you today?'}, log='Well, hello there! What can I do for you today?')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"Hi!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='search_wikipedia', tool_input={'query': 'LangChain'}, log=\"\\nInvoking: `search_wikipedia` with `{'query': 'LangChain'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"LangChain\"}', 'name': 'search_wikipedia'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 64, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_86d0290411', 'id': 'chatcmpl-BFvX9GNec05ToRrHRs8macGI0guif', 'finish_reason': 'function_call', 'logprobs': None}, id='run-bf4d32b4-0598-491c-aa52-9208f5e50283-0', usage_metadata={'input_tokens': 64, 'output_tokens': 17, 'total_tokens': 81, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is langchain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 동적으로 대화를 이어가기 위해 prompt에 `placeholder` 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_with_placeholder = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are helpful but sassy assistant\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_with_placeholder | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_wikipedia {'query': 'LangChain'}\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"What is langchain?\", \"agent_scratchpad\": []})\n",
    "print(result.tool, result.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47657/2910850974.py:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  observation = search_wikipedia(result.tool_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LangChain]\n",
      "LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n"
     ]
    }
   ],
   "source": [
    "observation = search_wikipedia(result.tool_input)\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Langchain\"}', 'name': 'search_wikipedia'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 64, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_86d0290411', 'id': 'chatcmpl-BFvafSf7ovcfDVuxYOeL4Mx2nGgMw', 'finish_reason': 'function_call', 'logprobs': None}, id='run-361cf514-7aa0-4bc5-8a3f-de98f395c26f-0', usage_metadata={'input_tokens': 64, 'output_tokens': 17, 'total_tokens': 81, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " FunctionMessage(content=\"[LangChain]\\nLangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\", additional_kwargs={}, response_metadata={}, name='search_wikipedia')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents.format_scratchpad.openai_functions import (\n",
    "    format_to_openai_functions,\n",
    ")\n",
    "\n",
    "format_to_openai_functions(\n",
    "    [\n",
    "        (result, observation),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': \"Oh, you want recommendations based on LangChain? Well, if you're diving into the world of large language models, you might want to explore applications like document analysis, chatbots, or even code analysis. Those are pretty hot right now! Just make sure you don't get lost in the tech jargon—it's easy to do!\"}, log=\"Oh, you want recommendations based on LangChain? Well, if you're diving into the world of large language models, you might want to explore applications like document analysis, chatbots, or even code analysis. Those are pretty hot right now! Just make sure you don't get lost in the tech jargon—it's easy to do!\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = chain.invoke(\n",
    "    {\n",
    "        \"input\": \"Do you have any recommendation for the previous result?\",\n",
    "        \"agent_scratchpad\": format_to_openai_functions([(result, observation)]),\n",
    "    }\n",
    ")\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "\n",
    "def run_agent_simple(user_input):\n",
    "    intermediate_steps = []\n",
    "\n",
    "    while True:\n",
    "        result = chain.invoke(\n",
    "            {\n",
    "                \"input\": user_input,\n",
    "                \"agent_scratchpad\": format_to_openai_functions(intermediate_steps),\n",
    "            }\n",
    "        )\n",
    "        print(result)\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "\n",
    "        observation = eval(result.tool)(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "result = chain.invoke({\n",
    "    'input': user_input,\n",
    "    'agent_scratchpad': format_to_openai_functions(intermediate_steps)\n",
    "})\n",
    "```\n",
    "\n",
    "부분을 단순화시키기 위해 `RunnablePassthrough`를 이용하여 두 부분으로 나눌 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
    "\n",
    "\n",
    "agent_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "    )\n",
    "    | chain\n",
    ")\n",
    "\n",
    "\n",
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    prev_result = None\n",
    "\n",
    "    while True:\n",
    "        cur_result = agent_chain.invoke(\n",
    "            {\n",
    "                \"input\": user_input,\n",
    "                \"intermediate_steps\": intermediate_steps,  # agent_scratchpad 부분을 바꿔주었음\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if isinstance(cur_result, AgentFinish):\n",
    "            return cur_result.return_values\n",
    "        elif cur_result == prev_result:\n",
    "            return observation\n",
    "\n",
    "        observation = eval(cur_result.tool)(cur_result.tool_input)\n",
    "        intermediate_steps.append((cur_result, observation))\n",
    "        prev_result = cur_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'Well, hello there! What can I do for you today?'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `AgentFinish`가 발생하지 않고 계속 동일한 결과가 나오는 경우를 대비하여 `cur_result == prev_result` 조건을 추가 \\\n",
    "  Prompt에 따라 발생하지 않을 수도 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': \"LangChain is a software framework designed to make it easier to integrate large language models (LLMs) into applications. Its use cases include document analysis, summarization, chatbots, and code analysis. So, if you're looking to sprinkle some AI magic into your app, LangChain might just be your new best friend!\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"What is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': \"Oh, absolutely! LangChain is a software framework designed to make it easier to integrate large language models (LLMs) into various applications. It covers a range of use cases, like document analysis, summarization, chatbots, and even code analysis. So, if you're looking to sprinkle some AI magic into your app, LangChain might just be your best friend!\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"Do you know what is langchain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 AgentExecutor\n",
    "\n",
    "langchain에서 지원하는 AgentExecutor를 사용하면 좀 더 fancy하게 사용할 수 있다.\n",
    "\n",
    "- `intermediate_steps` key를 사용하기 때문에 이를 넘겨줄 수 있어야한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt_with_placeholder = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are helpful but sassy assistant\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "    )\n",
    "    | prompt_with_placeholder\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_wikipedia` with `{'query': 'LangChain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[LangChain]\n",
      "LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\u001b[0m\u001b[32;1m\u001b[1;3mLangChain is a software framework designed to make it easier to integrate large language models (LLMs) into applications. Its use cases include document analysis, summarization, chatbots, and code analysis. So, if you're looking to sprinkle some AI magic into your app, LangChain might just be your new best friend!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is langchain?',\n",
       " 'output': \"LangChain is a software framework designed to make it easier to integrate large language models (LLMs) into applications. Its use cases include document analysis, summarization, chatbots, and code analysis. So, if you're looking to sprinkle some AI magic into your app, LangChain might just be your new best friend!\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents.agent import AgentExecutor\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent_chain, tools=[search_wikipedia], verbose=True\n",
    ")\n",
    "agent_executor.invoke({\"input\": \"What is langchain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 MemoryBuffer\n",
    "\n",
    "과거 대화를 기억하기 위해 memory를 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47657/747638168.py:13: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')  # return_messages=True: return string\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are helpful but sassy assistant\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "agent_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, memory_key=\"chat_history\"\n",
    ")  # return_messages=True: return string\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent_chain, tools=[search_wikipedia], verbose=True, memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHey Bob! I'm your sassy assistant, but you can just call me Assistant. What can I help you with today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Hello, my name is Bob. What's your name?\",\n",
       " 'chat_history': [HumanMessage(content=\"Hello, my name is Bob. What's your name?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Bob! I'm your sassy assistant, but you can just call me Assistant. What can I help you with today?\", additional_kwargs={}, response_metadata={})],\n",
       " 'output': \"Hey Bob! I'm your sassy assistant, but you can just call me Assistant. What can I help you with today?\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Hello, my name is Bob. What's your name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_wikipedia` with `{'query': 'LangChain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[LangChain]\n",
      "LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\u001b[0m\u001b[32;1m\u001b[1;3mLangChain is a software framework designed to make it easier to integrate large language models (LLMs) into applications. It’s pretty versatile and can be used for various tasks like document analysis, summarization, chatbots, and even code analysis. So, if you're looking to sprinkle some AI magic into your app, LangChain might just be your best friend! Need more details, or are you ready to dive into the world of LLMs?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Do you know what is langchain?',\n",
       " 'chat_history': [HumanMessage(content=\"Hello, my name is Bob. What's your name?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Bob! I'm your sassy assistant, but you can just call me Assistant. What can I help you with today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Do you know what is langchain?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"LangChain is a software framework designed to make it easier to integrate large language models (LLMs) into applications. It’s pretty versatile and can be used for various tasks like document analysis, summarization, chatbots, and even code analysis. So, if you're looking to sprinkle some AI magic into your app, LangChain might just be your best friend! Need more details, or are you ready to dive into the world of LLMs?\", additional_kwargs={}, response_metadata={})],\n",
       " 'output': \"LangChain is a software framework designed to make it easier to integrate large language models (LLMs) into applications. It’s pretty versatile and can be used for various tasks like document analysis, summarization, chatbots, and even code analysis. So, if you're looking to sprinkle some AI magic into your app, LangChain might just be your best friend! Need more details, or are you ready to dive into the world of LLMs?\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Do you know what is langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mOf course, Bob! Your name is hard to forget, especially when you introduce yourself like a star. What else can I do for you?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"OK good. So, do you know what's my name?\",\n",
       " 'chat_history': [HumanMessage(content=\"Hello, my name is Bob. What's your name?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Bob! I'm your sassy assistant, but you can just call me Assistant. What can I help you with today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Do you know what is langchain?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"LangChain is a software framework designed to make it easier to integrate large language models (LLMs) into applications. It’s pretty versatile and can be used for various tasks like document analysis, summarization, chatbots, and even code analysis. So, if you're looking to sprinkle some AI magic into your app, LangChain might just be your best friend! Need more details, or are you ready to dive into the world of LLMs?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"OK good. So, do you know what's my name?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Of course, Bob! Your name is hard to forget, especially when you introduce yourself like a star. What else can I do for you?', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'Of course, Bob! Your name is hard to forget, especially when you introduce yourself like a star. What else can I do for you?'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"OK good. So, do you know what's my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Final Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m안녕, 지원! 나는 너의 똑똑하고 약간 시크한 어시스턴트야. 뭐 도와줄까?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = agent_executor.invoke({\"input\": \"안녕. 내 이름은 지원이야. 네 이름은 뭐야?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_wikipedia` with `{'query': '행복한 삶을 사는 방법'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3m찾아보려고 했지만, 행복한 삶을 사는 방법에 대한 특별한 위키피디아 기사는 없더라고. 하지만 내가 몇 가지 팁을 줄 수 있어!\n",
      "\n",
      "1. **감사하는 마음**: 매일 감사한 것들을 적어보면 긍정적인 마음가짐을 유지하는 데 도움이 돼.\n",
      "2. **건강한 관계**: 친구와 가족과의 좋은 관계는 행복의 큰 원천이야.\n",
      "3. **취미 생활**: 좋아하는 일을 하면서 스트레스를 풀고 즐거움을 느껴봐.\n",
      "4. **운동**: 신체 활동은 기분을 좋게 하고 에너지를 높여줘.\n",
      "5. **목표 설정**: 작은 목표를 세우고 달성하는 기쁨을 느껴봐.\n",
      "\n",
      "이런 것들이 행복한 삶을 사는 데 도움이 될 거야! 더 궁금한 점이 있으면 언제든지 물어봐!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = agent_executor.invoke({\"input\": \"행복한 삶을 사는 방법에 대해 알려줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_wikipedia` with `{'query': 'LangChain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[LangChain]\n",
      "LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\u001b[0m\u001b[32;1m\u001b[1;3mLangChain은 대형 언어 모델(LLM)을 애플리케이션에 통합하는 데 도움을 주는 소프트웨어 프레임워크야. 이 프레임워크는 문서 분석, 요약, 챗봇, 코드 분석 등 다양한 용도로 사용될 수 있어. 언어 모델을 활용하고 싶다면 LangChain이 유용할 거야! 더 궁금한 점이 있으면 언제든지 물어봐!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = agent_executor.invoke({\"input\": \"Langchain 이 뭔지 아니?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m물론이지, 지원! 네 이름은 잊지 않을 거야. 또 궁금한 거 있으면 언제든지 물어봐!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = agent_executor.invoke({\"input\": \"고마워. 내 이름이 뭔지 기억해?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m고마워, 지원! 나는 많은 데이터와 정보를 바탕으로 훈련받았거든. 그래서 이렇게 똑똑하게 대답할 수 있어! 너의 질문에 맞춰서 최선을 다하고 있어. 더 궁금한 점이 있으면 언제든지 물어봐!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = agent_executor.invoke({\"input\": \"너 말을 되게 잘한다. 어디서 그렇게 배웠어?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
